#######################################################################
# Test for copying block of size 63;
#######################################################################
	.pos 0
main:	irmovq Stack, %rsp  	# Set up stack pointer

	# Set up arguments for copy function and then invoke it
	irmovq $63, %rdx		# src and dst have 63 elements
	irmovq dest, %rsi	# dst array
	irmovq src, %rdi	# src array
	call ncopy		 
	halt			# should halt with num nonzeros in %rax
StartFun:
#/* $begin ncopy-ys */
##################################################################
# ncopy.ys - Copy a src block of len words to dst.
# Return the number of positive words (>0) contained in src.
#
# Include your name and ID here.
#
# Tanziming 520021910607
#
# Describe how and why you modified the baseline code.
#
# 1. use iaddq:
#		using iaddq V,rB will combine two instructions irmovq and addq
#		into one step. Since there are many ++, -- operations in ys code,
#		this change will be of great help. So I modify pipe-full.hcl to
#		add iaddq and change all instruction with continously irmovq and
#		addq to iaddq in ncopy.ys.
#
# 2. change order to remove instruction:
#		For these lines below 
#				iaddq $-1, %rdx
#				......
#				andq %rdx,%rdx
#				jg Loop
#		The order of ... and iaddq $-1, %rdx does not matter, so we can
#		change the order of them to
#				......
#				iaddq $-1, %rdx
#				andq %rdx,%rdx  # then we can remove this line
#				jg Loop
#		Considering we have already get the correct CC, we can remove last line,
#		which means we reduce 1 step each loop.
#
# 3. remove xorq %rax,%rax
#		Since the initial value of %rax is 0, we can simply remove this line at head.
#
# 4. loop unrolling:
#		Since there is a loop in function, we can adopt loop unrolling
#		to decrease loop times. After trying unlooping for 4,5,6,7 and
#		8 times, I find that 6 times is the most efficient. Therefore I
#		unloop for 6 times at last.
#
# 5. avoid load-use hazard in loop:
#		For these lines below 
#				mrmovq (%rdi), %r10
#				rmmovq %r10, (%rsi)
#		every time going through these two lines, we will have load-use
#		hazard and generate one bubble, wasting one cycle. So we can change 
#		order of instruction to make sure one other instruction between 
#		these two. Then we can avoid load-use hazard.
#				mrmovq (%rdi), %r10	# read val from src...
#				mrmovq 8(%rdi), %r9	# load one more ahead
#				rmmovq %r10, (%rsi)	# ...and store it to dst
#		So I load one more ahead and store them in two different register.
#		
# 6. try to use jump table for the rest
#
# 7. avoid load-use hazard for the rest: ?useless
#
# 
#
##################################################################
# Do not modify this portion
# Function prologue.
# %rdi = src, %rsi = dst, %rdx = len
ncopy:

##################################################################
# You can modify this portion
	# Loop header
test:  				# test len >= 8
	iaddq $-8, %rdx
	jge Loop			# if len >= 8, goto Loop:

# jump table
	addq %rdx, %rdx
	addq %rdx, %rdx
	addq %rdx, %rdx
	mrmovq restTable(%rdx),%r8
	pushq %r8
	ret

# unloop for 6 times when len >= 6
Loop:	
	mrmovq (%rdi), %r10	
	mrmovq 8(%rdi), %r9	
	rmmovq %r10, (%rsi)
	andq %r10, %r10	
	jle Loop1
	iaddq $1, %rax 
Loop1:	
	mrmovq 16(%rdi), %r10
	rmmovq %r9, 8(%rsi)
	andq %r9, %r9
	jle Loop2
	iaddq $1, %rax
Loop2:	
	mrmovq 24(%rdi), %r9
	rmmovq %r10, 16(%rsi)
	andq %r10, %r10	
	jle Loop3
	iaddq $1, %rax 
Loop3:	
	mrmovq 32(%rdi), %r10
	rmmovq %r9, 24(%rsi)
	andq %r9, %r9
	jle Loop4
	iaddq $1, %rax
Loop4:	
	mrmovq 40(%rdi), %r9
	rmmovq %r10, 32(%rsi)
	andq %r10, %r10	
	jle Loop5
	iaddq $1, %rax
Loop5:	
	mrmovq 48(%rdi), %r10
	rmmovq %r9, 40(%rsi)
	andq %r9, %r9
	jle Loop6
	iaddq $1, %rax
Loop6:	
	mrmovq 56(%rdi), %r9
	rmmovq %r10, 48(%rsi)
	andq %r10, %r10	
	jle Loop7
	iaddq $1, %rax  
Loop7:	
	rmmovq %r9, 56(%rsi)
	iaddq $64, %rdi
	iaddq $64, %rsi	
	andq %r9, %r9	
	jle test		
	iaddq $1, %rax 
	jmp test

# for rest of len
rest7:
	mrmovq 48(%rdi), %r10
	rmmovq %r10, 48(%rsi)
	andq %r10, %r10
	jle rest6
	iaddq $1, %rax
rest6:
	mrmovq 40(%rdi), %r10
	rmmovq %r10, 40(%rsi)
	andq %r10, %r10
	jle rest5
	iaddq $1, %rax
rest5:
	mrmovq 32(%rdi), %r10
	rmmovq %r10, 32(%rsi)
	andq %r10, %r10
	jle rest4
	iaddq $1, %rax
rest4:
	mrmovq 24(%rdi), %r10
	rmmovq %r10, 24(%rsi)
	andq %r10, %r10
	jle rest3	
	iaddq $1, %rax
rest3:
	mrmovq 16(%rdi), %r10
	rmmovq %r10, 16(%rsi)
	andq %r10, %r10
	jle rest2
	iaddq $1, %rax
rest2:
	mrmovq 8(%rdi), %r10
	rmmovq %r10, 8(%rsi)
	andq %r10, %r10
	jle rest1
	iaddq $1, %rax
rest1:
	mrmovq (%rdi), %r10
	rmmovq %r10, (%rsi)
	andq %r10, %r10
	jle Npos
	iaddq $1, %rax
Npos:
	ret

	.align 8
	.quad Done
	.quad rest1
	.quad rest2
	.quad rest3
	.quad rest4
	.quad rest5
	.quad rest6
	.quad rest7
restTable:

##################################################################
# Do not modify the following section of code
# Function epilogue.
Done:
	ret
##################################################################
# Keep the following label at the end of your function
End:
#/* $end ncopy-ys */
EndFun:

###############################
# Source and destination blocks 
###############################
	.align 8
src:
	.quad -1
	.quad -2
	.quad 3
	.quad 4
	.quad -5
	.quad 6
	.quad 7
	.quad 8
	.quad 9
	.quad -10
	.quad -11
	.quad 12
	.quad -13
	.quad 14
	.quad -15
	.quad 16
	.quad -17
	.quad 18
	.quad -19
	.quad -20
	.quad 21
	.quad 22
	.quad -23
	.quad 24
	.quad 25
	.quad 26
	.quad -27
	.quad 28
	.quad 29
	.quad -30
	.quad 31
	.quad 32
	.quad -33
	.quad -34
	.quad 35
	.quad 36
	.quad 37
	.quad 38
	.quad 39
	.quad 40
	.quad 41
	.quad -42
	.quad 43
	.quad -44
	.quad 45
	.quad -46
	.quad 47
	.quad -48
	.quad 49
	.quad 50
	.quad -51
	.quad -52
	.quad -53
	.quad -54
	.quad -55
	.quad -56
	.quad -57
	.quad -58
	.quad -59
	.quad -60
	.quad -61
	.quad -62
	.quad -63
	.quad 0xbcdefa # This shouldn't get moved

	.align 16
Predest:
	.quad 0xbcdefa
dest:
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
	.quad 0xcdefab
Postdest:
	.quad 0xdefabc

.align 8
# Run time stack
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0
	.quad 0

Stack:
